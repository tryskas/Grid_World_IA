# -*- coding: utf-8 -*-
"""IA_IntroRL_ENSISA_2A_TD1_Prbl2 (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18Zj4dElJkeRI8gngX-u-29UyLQ00Kxh_

> Import libraries to use
"""

import numpy as np

""">  # Introduction to numpy (Skip if you already are familiar)

>> Creating a 1D array
"""

a = np.array([1,2,3,4])
print(a)

""">> Creating a 2D array

"""

a = np.array([[1,2],[3,4]])
print(a)

""">> Creating an array full of zeros

"""

a = np.zeros(shape=(10))
print(a)
a = np.zeros(shape=(5,2))
print(a)

""">> Infinity in numpy"""

print(np.inf)

""">> Max and Argmax"""

a = np.array([2,1,4,3])
print(np.max(a))
print(np.argmax(a))

""">> From list to Numpy"""

l = [1,2,3,4]
print(l)
print(np.asarray(l))

""">> Random in numpy"""

# Array of Random integers ranging from 1 to 10 (with any size you want)
a = np.random.randint(low=1, high=10, size=(5,2))
print(a)

# Array of random elements of a list with any size you want
a = np.random.choice([0,1,2], size=(2,))

""">> Shapes in numpy"""

a = np.random.randint(low=1, high=5, size=(4,2))
print(a.shape)
print(a)

# Reshape a to a vector of shape = (8,1)
a = a.reshape((8,1))
print(a.shape)
print(a)

"""# Pre-defined utilities"""

int_to_char = {
    0 : 'u',
    1 : 'r',
    2 : 'd',
    3 : 'l'
}

policy_one_step_look_ahead = {
    0 : [-1,0],
    1 : [0,1],
    2 : [1,0],
    3 : [0,-1]
}

def policy_int_to_char(pi,n):

    pi_char = ['']

    for i in range(n):
        for j in range(n):

            if i == 0 and j == 0 or i == n-1 and j == n-1:

                continue

            pi_char.append(int_to_char[pi[i,j]])

    pi_char.append('')

    return np.asarray(pi_char).reshape(n,n)

"""# 1- Policy evaluation"""

def policy_evaluation(n, pi, v, Gamma, threshold, max_iterations=1000):
    """
    This function should return the value function that follows the policy pi.
    Use the stopping criteria given in the problem statement.

    Parameters:
    n: The size of the grid or state space.
    pi: The policy to be evaluated.
    v: The current value function.
    Gamma: The discount factor.
    threshold: The threshold for the stopping criteria.
    """
    policy_one_step_look_ahead = {
        0: (-1, 0),  # Up
        1: (0, 1),   # Right
        2: (1, 0),   # Down
        3: (0, -1)   # Left
    }

    iteration = 0

    while True:
        v_old = v.copy()
        for i in range(n):
            for j in range(n):

                if (i == 0 and j == 0) or (i == n - 1 and j == n - 1):
                    v[i, j] = 0
                    continue

                action = pi[i, j]
                next_state = (i + policy_one_step_look_ahead[action][0], j + policy_one_step_look_ahead[action][1])

                if 0 <= next_state[0] < n and 0 <= next_state[1] < n:
                    v[i, j] = -1 + Gamma * v_old[next_state[0], next_state[1]]
                else:
                    v[i, j] = -1  # Out of bounds

        max_diff = np.max(np.abs(v - v_old))
        if max_diff <= threshold or iteration >= max_iterations:
            break

    return v

"""# 2- Policy improvement"""

def policy_improvement(n, pi, v, Gamma):
    """
    This function should return the new policy by acting in a greedy manner.
    The function should return as well a flag indicating if the output policy
    is the same as the input policy.

    Example:
      return new_pi, True if new_pi = pi for all states
      else return new_pi, False
    """
    new_pi = np.zeros_like(pi)
    policy_stable = True

    policy_one_step_look_ahead = {
        0: (-1, 0),  # Up
        1: (0, 1),   # Right
        2: (1, 0),   # Down
        3: (0, -1)   # Left
    }

    for i in range(n):
        for j in range(n):
            state = i * n + j
            if state == 0 or state == n * n - 1:
                new_pi[i, j] = pi[i, j]
                continue

            old_action = pi[i, j]
            action_values = np.zeros(4)

            for action in range(4):
                next_state = (i + policy_one_step_look_ahead[action][0], j + policy_one_step_look_ahead[action][1])
                if 0 <= next_state[0] < n and 0 <= next_state[1] < n:
                    action_values[action] = -1 + Gamma * v[next_state[0], next_state[1]]
                else:
                    action_values[action] = -1

            new_action = np.argmax(action_values)
            new_pi[i, j] = new_action

            if new_action != old_action:
                policy_stable = False

    return new_pi, policy_stable

"""# 3- Policy Initialization"""

def policy_initialization(n):
  """
    This function should return the initial random policy for all states.
  """
  # Define the possible actions (0: 'u', 1: 'r', 2: 'd', 3: 'l')
  actions = [0, 1, 2, 3]

  # Initialize a random policy for each state
  pi = np.random.choice(actions, size=(n, n))

  return pi

"""# 4- Policy Iteration algorithm"""

def policy_iteration(n,Gamma,threshhold):

    pi = policy_initialization(n=n)

    v = np.zeros(shape=(n,n))

    iteration = 0

    while True:

        iteration += 1

        v = policy_evaluation(n=n,v=v,pi=pi,threshold=threshhold,Gamma=Gamma)

        print(f"Iteration {iteration}: Value function updated")

        pi , pi_stable = policy_improvement(n=n,pi=pi,v=v,Gamma=Gamma)

        print(f"Iteration {iteration}: Policy updated, stable: {pi_stable}")

        if pi_stable:

            break

        if iteration > 1000:
            print("Maximum iterations reached. Policy might not have converged.")
            break

    return pi , v

"""# Main Code to Test"""

def policy_int_to_char(n, pi):
  """
  Converts a policy represented by integers to characters.

  Args:
    n: The size of the grid.
    pi: The policy represented by integers (0: 'u', 1: 'r', 2: 'd', 3: 'l').

  Returns:
    A grid of characters representing the policy.
  """

  # Define a mapping from integers to characters
  action_map = {0: 'u', 1: 'r', 2: 'd', 3: 'l'}

  # Convert the policy to characters
  pi_char = np.empty(shape=(n, n), dtype=str)
  for i in range(n):
    for j in range(n):
      pi_char[i, j] = action_map[pi[i, j]]

  return pi_char

n = 4

Gamma = [0.8,0.9,1]

threshhold = 1e-4

for _gamma in Gamma:

    print(f"Gamma = {_gamma}")
    pi , v = policy_iteration(n=n,Gamma=_gamma,threshhold=threshhold)
    pi_char = policy_int_to_char(n=n,pi=pi)

    print("Optimal Policy (Character Representation):")
    print(pi_char)
    print("Optimal Value Function:")
    print(v)